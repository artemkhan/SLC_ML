{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0073e987-eb22-4189-b597-8856e651b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 16:04:03.836822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-16 16:04:04.526916: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-16 16:04:04.908495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Import additional libraries for Random Forest and SVM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3550f6ba-bef5-43d3-b335-320513c0eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re==2.2.1\n",
      "ipaddress==1.0\n",
      "ipykernel._version==6.29.5\n",
      "json==2.0.9\n",
      "jupyter_client._version==8.6.2\n",
      "platform==1.0.8\n",
      "zmq.sugar.version==26.1.0\n",
      "zmq.sugar==26.1.0\n",
      "zmq==26.1.0\n",
      "logging==0.5.1.2\n",
      "traitlets._version==5.14.3\n",
      "traitlets==5.14.3\n",
      "jupyter_core.version==5.7.2\n",
      "jupyter_core==5.7.2\n",
      "zlib==1.0\n",
      "_curses==b'2.2'\n",
      "socketserver==0.4\n",
      "argparse==1.1\n",
      "dateutil._version==2.9.0\n",
      "dateutil==2.9.0\n",
      "six==1.16.0\n",
      "_decimal==1.70\n",
      "decimal==1.70\n",
      "platformdirs.version==4.2.2\n",
      "platformdirs==4.2.2\n",
      "_csv==1.0\n",
      "csv==1.0\n",
      "jupyter_client==8.6.2\n",
      "ipykernel==6.29.5\n",
      "IPython.core.release==8.26.0\n",
      "executing.version==2.0.1\n",
      "executing==2.0.1\n",
      "pure_eval.version==0.2.3\n",
      "pure_eval==0.2.3\n",
      "stack_data.version==0.6.2\n",
      "stack_data==0.6.2\n",
      "pygments==2.18.0\n",
      "pickleshare==0.7.5\n",
      "decorator==5.1.1\n",
      "wcwidth==0.2.13\n",
      "prompt_toolkit==3.0.47\n",
      "parso==0.8.4\n",
      "jedi==0.19.1\n",
      "urllib.request==3.12\n",
      "IPython==8.26.0\n",
      "comm==0.2.2\n",
      "psutil==6.0.0\n",
      "packaging==24.1\n",
      "_ctypes==1.1.0\n",
      "ctypes==1.1.0\n",
      "debugpy.public_api==1.8.5\n",
      "debugpy==1.8.5\n",
      "xmlrpc.client==3.12\n",
      "http.server==0.6\n",
      "_pydevd_frame_eval.vendored.bytecode==0.13.0.dev\n",
      "_pydev_bundle.fsnotify==0.1.5\n",
      "pydevd==2.9.5\n",
      "numpy.version==1.26.4\n",
      "numpy.core._multiarray_umath==3.1\n",
      "numpy.core==1.26.4\n",
      "numpy.linalg._umath_linalg==0.1.5\n",
      "numpy==1.26.4\n",
      "pytz==2024.1\n",
      "pyarrow._generated_version==17.0.0\n",
      "numpy._core._multiarray_umath==3.1\n",
      "pyarrow==17.0.0\n",
      "pandas._version_meson==2.2.2\n",
      "pandas==2.2.2\n",
      "setuptools._distutils==3.12.5\n",
      "distutils._vendor.packaging==24.0\n",
      "setuptools.version==72.1.0\n",
      "more_itertools==10.3.0\n",
      "ordered_set==4.1.0\n",
      "setuptools==72.1.0\n",
      "distutils==3.12.5\n",
      "google.protobuf==4.25.4\n",
      "tensorflow.python.client.pywrap_tf_session==2.17.0\n",
      "ml_dtypes==0.4.1\n",
      "wrapt==1.16.0\n",
      "tensorflow.python.framework.versions==2.17.0\n",
      "gast.version==0.6.0\n",
      "gast==0.6.0\n",
      "astunparse==1.6.3\n",
      "opt_einsum==v3.3.0\n",
      "dill.__info__==0.3.8\n",
      "dill==0.3.8\n",
      "_brotli==1.1.0\n",
      "brotli==1.1.0\n",
      "zstandard.backend_c==0.23.0\n",
      "zstandard==0.23.0\n",
      "urllib3._version==2.2.2\n",
      "urllib3.util.ssl_match_hostname==3.5.0.1\n",
      "urllib3.connection==2.2.2\n",
      "urllib3==2.2.2\n",
      "charset_normalizer.version==3.3.2\n",
      "charset_normalizer==3.3.2\n",
      "requests.packages.urllib3._version==2.2.2\n",
      "requests.packages.urllib3.util.ssl_match_hostname==3.5.0.1\n",
      "requests.packages.urllib3.connection==2.2.2\n",
      "requests.packages.urllib3==2.2.2\n",
      "idna.package_data==3.7\n",
      "idna.idnadata==15.1.0\n",
      "idna==3.7\n",
      "requests.packages.idna.package_data==3.7\n",
      "requests.packages.idna.idnadata==15.1.0\n",
      "requests.packages.idna==3.7\n",
      "requests.packages.charset_normalizer.version==3.3.2\n",
      "requests.packages.chardet.version==3.3.2\n",
      "requests.packages.charset_normalizer==3.3.2\n",
      "requests.packages.chardet==3.3.2\n",
      "certifi==2024.07.04\n",
      "requests.__version__==2.32.3\n",
      "requests.utils==2.32.3\n",
      "socks==1.7.1\n",
      "requests==2.32.3\n",
      "h5py==3.11.0\n",
      "scipy==1.14.0\n",
      "scipy._lib.array_api_compat==1.5.1\n",
      "scipy._lib.array_api_compat.numpy==1.26.4\n",
      "scipy.linalg._fblas==2.0.0\n",
      "scipy.linalg._flapack==2.0.0\n",
      "scipy._lib.decorator==4.0.5\n",
      "scipy.sparse.linalg._eigen.arpack._arpack==2.0.0\n",
      "scipy.sparse.linalg._propack._spropack==2.0.0\n",
      "scipy.sparse.linalg._propack._dpropack==2.0.0\n",
      "scipy.sparse.linalg._propack._cpropack==2.0.0\n",
      "scipy.sparse.linalg._propack._zpropack==2.0.0\n",
      "flatbuffers._version==24.3.25\n",
      "flatbuffers==24.3.25\n",
      "tensorflow._api.v2.compat.v1.compat.v1==2.17.0\n",
      "tensorflow._api.v2.compat.v2.compat.v1==2.17.0\n",
      "tensorflow._api.v2.compat.v2.compat.v2==2.17.0\n",
      "tensorflow._api.v2.compat.v2==2.17.0\n",
      "tensorflow._api.v2.compat.v1.compat.v2==2.17.0\n",
      "tensorflow._api.v2.compat.v1==2.17.0\n",
      "optree.version==0.12.1\n",
      "optree==0.12.1\n",
      "PIL._version==10.4.0\n",
      "PIL==10.4.0\n",
      "PIL._deprecate==10.4.0\n",
      "defusedxml==0.7.1\n",
      "cffi==1.17.0\n",
      "PIL.Image==10.4.0\n",
      "attr==24.2.0\n",
      "keras.src.version==3.5.0\n",
      "huggingface_hub==0.24.5\n",
      "keras.src==3.5.0\n",
      "keras.api==3.5.0\n",
      "keras==3.5.0\n",
      "tensorflow==2.17.0\n",
      "keras._tf_keras.keras==3.5.0\n",
      "tensorflow.keras==3.5.0\n",
      "joblib.externals.cloudpickle==3.0.0\n",
      "joblib.externals.loky==3.4.1\n",
      "joblib==1.4.2\n",
      "sklearn.utils._joblib==1.4.2\n",
      "scipy.optimize._lbfgsb==2.0.0\n",
      "scipy.optimize._cobyla==2.0.0\n",
      "scipy.optimize._slsqp==2.0.0\n",
      "scipy.linalg._interpolative==2.0.0\n",
      "scipy.integrate._vode==2.0.0\n",
      "scipy.integrate._dop==2.0.0\n",
      "scipy.integrate._lsoda==2.0.0\n",
      "scipy.interpolate._dfitpack==2.0.0\n",
      "scipy._lib._uarray==0.8.8.dev0+aa94c5a4.scipy\n",
      "scipy.stats._mvn==2.0.0\n",
      "threadpoolctl==3.5.0\n",
      "sklearn.utils._estimator_html_repr==1.5.1\n",
      "sklearn.base==1.5.1\n",
      "sklearn.utils._show_versions==1.5.1\n",
      "sklearn==1.5.1\n",
      "pyparsing==3.1.2\n",
      "cycler==0.12.1\n",
      "kiwisolver._cext==1.4.5\n",
      "kiwisolver==1.4.5\n",
      "matplotlib==3.9.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import types\n",
    "\n",
    "for name, val in sys.modules.items():\n",
    "    if isinstance(val, types.ModuleType) and hasattr(val, '__version__'):\n",
    "        try:\n",
    "            print(f\"{name}=={val.__version__}\")\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb4fd4c-e338-4946-ac98-f294505e7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set a seed for numpy (used in arrays, random number generation)\n",
    "np.random.seed(123)\n",
    "\n",
    "# 2. Set a seed for Python's built-in random module\n",
    "random.seed(123)\n",
    "\n",
    "# 3. Set a seed for TensorFlow/Keras (if using neural networks)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b006fcc-009f-4532-a5fe-e4dbb2419540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TensorFlow, you can also disable GPU and other non-deterministic behaviors (optional, based on the use case)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5224e3-6565-4069-9250-7a83177e396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('20240923_TrainingDataSet_Clean.csv')\n",
    "feature_columns = data.columns.drop(['Unnamed: 0', 'target', 'CADD_METSIM'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "data['target'] = data['target'].replace(-1, 0)\n",
    "data = data.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fe00346d-e3c7-4039-b515-e270679280e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement 5-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=123)\n",
    "\n",
    "# Define reduced hyperparameter grid for neural network\n",
    "param_grid = {\n",
    "    'units1': [32, 64],\n",
    "    'units2': [16, 32],\n",
    "    'dropout_rate': [0.2],     # Fixed dropout rate\n",
    "    'batch_size': [32],        # Fixed batch size\n",
    "    'epochs': [50],            # Fixed number of epochs\n",
    "    'lr': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "hyperparameter_list = list(product(\n",
    "    param_grid['units1'],\n",
    "    param_grid['units2'],\n",
    "    param_grid['dropout_rate'],\n",
    "    param_grid['batch_size'],\n",
    "    param_grid['epochs'],\n",
    "    param_grid['lr']\n",
    "))\n",
    "\n",
    "# Initialize results list for neural network\n",
    "nn_results_list = []\n",
    "\n",
    "# Initialize results lists for Random Forest and SVM\n",
    "rf_metrics_list = []\n",
    "svm_metrics_list = []\n",
    "# Initialize the list to store Gradient Boosting metrics\n",
    "gb_metrics_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "49164ddb-cb1a-491a-b50b-7f114efc1764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Fold 1\n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Random Forest Accuracy: 0.6579, F1 Score: 0.5991, AUC: 0.7007\n",
      "  Gradient Boosting Accuracy: 0.6466, F1 Score: 0.6148, AUC: 0.6992\n",
      "  SVM Accuracy: 0.6128, F1 Score: 0.5961, AUC: 0.6277\n",
      "\n",
      "Processing Fold 2\n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Random Forest Accuracy: 0.6917, F1 Score: 0.6639, AUC: 0.7520\n",
      "  Gradient Boosting Accuracy: 0.6880, F1 Score: 0.6770, AUC: 0.7620\n",
      "  SVM Accuracy: 0.6541, F1 Score: 0.6642, AUC: 0.6840\n",
      "\n",
      "Processing Fold 3\n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Random Forest Accuracy: 0.7068, F1 Score: 0.6695, AUC: 0.7511\n",
      "  Gradient Boosting Accuracy: 0.6805, F1 Score: 0.6502, AUC: 0.7396\n",
      "  SVM Accuracy: 0.6165, F1 Score: 0.6194, AUC: 0.6792\n",
      "\n",
      "Processing Fold 4\n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Random Forest Accuracy: 0.7068, F1 Score: 0.6777, AUC: 0.7620\n",
      "  Gradient Boosting Accuracy: 0.6692, F1 Score: 0.6508, AUC: 0.7388\n",
      "  SVM Accuracy: 0.6842, F1 Score: 0.6818, AUC: 0.7476\n",
      "\n",
      "Processing Fold 5\n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 32, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 16, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.001\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Neural Network Hyperparameters: units1 = 64, units2 = 32, dropout_rate = 0.2, batch_size = 32, epochs = 50, lr = 0.01\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "  Random Forest Accuracy: 0.6541, F1 Score: 0.6462, AUC: 0.7110\n",
      "  Gradient Boosting Accuracy: 0.6692, F1 Score: 0.6667, AUC: 0.7045\n",
      "  SVM Accuracy: 0.6128, F1 Score: 0.6308, AUC: 0.6574\n"
     ]
    }
   ],
   "source": [
    "# Loop over folds\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(data):\n",
    "    print(f\"\\nProcessing Fold {fold}\")\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    training_data = data.iloc[train_index].reset_index(drop=True)\n",
    "    validation_data = data.iloc[val_index].reset_index(drop=True)\n",
    "    \n",
    "    # Impute missing values (median imputation)\n",
    "    # Calculate median of each feature in training data\n",
    "    medians = training_data[feature_columns].median()\n",
    "    \n",
    "    # Fill missing values in training data\n",
    "    training_data_imputed = training_data.copy()\n",
    "    training_data_imputed[feature_columns] = training_data_imputed[feature_columns].fillna(medians)\n",
    "    \n",
    "    # Fill missing values in validation data using training medians\n",
    "    validation_data_imputed = validation_data.copy()\n",
    "    validation_data_imputed[feature_columns] = validation_data_imputed[feature_columns].fillna(medians)\n",
    "    \n",
    "    # Separate features and target\n",
    "    x_train = training_data_imputed[feature_columns].values\n",
    "    y_train = training_data_imputed['target'].values\n",
    "    x_val = validation_data_imputed[feature_columns].values\n",
    "    y_val = validation_data_imputed['target'].values\n",
    "    \n",
    "    # Scale features using training data statistics\n",
    "    feature_means = x_train.mean(axis=0)\n",
    "    feature_stds = x_train.std(axis=0)\n",
    "    x_train_scaled = (x_train - feature_means) / feature_stds\n",
    "    x_val_scaled = (x_val - feature_means) / feature_stds  # Use training data stats\n",
    "\n",
    "    ### Neural Network ###\n",
    "    # Loop over hyperparameter grid\n",
    "    for params in hyperparameter_list:\n",
    "        units1, units2, dropout_rate, batch_size, epochs, lr = params\n",
    "        \n",
    "        print(f\"  Neural Network Hyperparameters: units1 = {units1}, units2 = {units2}, dropout_rate = {dropout_rate}, \"\n",
    "              f\"batch_size = {batch_size}, epochs = {epochs}, lr = {lr}\")\n",
    "        \n",
    "        # Build model\n",
    "        model = keras.Sequential([\n",
    "            keras.Input(shape=(x_train_scaled.shape[1],)),\n",
    "            layers.Dense(units1, activation='relu'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(units2, activation='relu'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Include early stopping\n",
    "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            x_train_scaled, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_val_scaled, y_val),\n",
    "            verbose=0,  # Set to 1 to see training progress\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Evaluate model on validation data\n",
    "        evaluation = model.evaluate(x_val_scaled, y_val, verbose=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_prob_nn = model.predict(x_val_scaled).flatten()\n",
    "        y_pred_nn = (y_pred_prob_nn >= 0.5).astype(int)\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        nn_f1 = f1_score(y_val, y_pred_nn)\n",
    "        nn_auc = roc_auc_score(y_val, y_pred_prob_nn)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred_nn).ravel()\n",
    "        nn_tpr = tp / (tp + fn)  # True Positive Rate\n",
    "        nn_tnr = tn / (tn + fp)  # True Negative Rate\n",
    "        nn_fpr = fp / (fp + tn)  # False Positive Rate\n",
    "        nn_fnr = fn / (fn + tp)  # False Negative Rate\n",
    "        \n",
    "        # Record results\n",
    "        nn_results_list.append({\n",
    "            'fold': fold,\n",
    "            'units1': units1,\n",
    "            'units2': units2,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs,\n",
    "            'lr': lr,\n",
    "            'trained_epochs': len(history.history['loss']),\n",
    "            'val_loss': evaluation[0],\n",
    "            'val_accuracy': evaluation[1],\n",
    "            'f1_score': nn_f1,\n",
    "            'auc': nn_auc,\n",
    "            'tp': tp,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'tpr': nn_tpr,\n",
    "            'tnr': nn_tnr,\n",
    "            'fpr': nn_fpr,\n",
    "            'fnr': nn_fnr\n",
    "        })\n",
    "    \n",
    "    ### Random Forest ###\n",
    "    # Instantiate Random Forest classifier with default hyperparameters\n",
    "    rf_model = RandomForestClassifier(random_state=123, min_samples_split=5, max_depth=10, n_estimators=200)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    rf_pred = rf_model.predict(x_val)\n",
    "    rf_pred_prob = rf_model.predict_proba(x_val)[:, 1]\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    rf_accuracy = accuracy_score(y_val, rf_pred)\n",
    "    rf_f1 = f1_score(y_val, rf_pred)\n",
    "    rf_auc = roc_auc_score(y_val, rf_pred_prob)\n",
    "   \n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, rf_pred).ravel()\n",
    "    rf_tpr = tp / (tp + fn)\n",
    "    rf_tnr = tn / (tn + fp)\n",
    "    rf_fpr = fp / (fp + tn)\n",
    "    rf_fnr = fn / (fn + tp)\n",
    "    rf_fpr_roc, rf_tpr_roc, thresholds = roc_curve(y_val, rf_pred_prob)\n",
    "    \n",
    "    # Find the index of the FPR desired\n",
    "    desired_fpr = 0.05\n",
    "    closest_idx = np.argmin(np.abs(rf_fpr_roc - desired_fpr))\n",
    "    optimal_threshold_FPR = thresholds[closest_idx]\n",
    "\n",
    "    # Record results\n",
    "    rf_metrics_list.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': rf_accuracy,\n",
    "        'f1_score': rf_f1,\n",
    "        'auc': rf_auc,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tpr': rf_tpr,\n",
    "        'tnr': rf_tnr,\n",
    "        'fpr': rf_fpr,\n",
    "        'fnr': rf_fnr,\n",
    "        'FPR_threshold': optimal_threshold_FPR\n",
    "    })\n",
    "\n",
    "    output_random_forest = pd.DataFrame({\n",
    "    'rf_fpr': rf_fpr_roc,\n",
    "    'rf_tpr': rf_tpr_roc,\n",
    "    'thresh': thresholds\n",
    "    })\n",
    "\n",
    "    output_random_forest.to_csv(f\"/output_random_forest_for_roc_{fold}.csv\", index=False)\n",
    "    print(f\"  Random Forest Accuracy: {rf_accuracy:.4f}, F1 Score: {rf_f1:.4f}, AUC: {rf_auc:.4f}\")\n",
    "\n",
    "    ### Gradient\n",
    "    # Instantiate Gradient Boosting classifier with default hyperparameters\n",
    "    gb_model = GradientBoostingClassifier(random_state=123)\n",
    "\n",
    "    # Train Gradient Boosting model\n",
    "    gb_model.fit(x_train, y_train)\n",
    "\n",
    "    # Predict on validation data\n",
    "    gb_pred = gb_model.predict(x_val)\n",
    "    gb_pred_prob = gb_model.predict_proba(x_val)[:, 1]\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    gb_accuracy = accuracy_score(y_val, gb_pred)\n",
    "    gb_f1 = f1_score(y_val, gb_pred)\n",
    "    gb_auc = roc_auc_score(y_val, gb_pred_prob)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, gb_pred).ravel()\n",
    "    gb_tpr = tp / (tp + fn)  # True Positive Rate (Recall)\n",
    "    gb_tnr = tn / (tn + fp)  # True Negative Rate\n",
    "    gb_fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    gb_fnr = fn / (fn + tp)  # False Negative Rate\n",
    "    \n",
    "    # Record results\n",
    "    gb_metrics_list.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': gb_accuracy,\n",
    "        'f1_score': gb_f1,\n",
    "        'auc': gb_auc,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tpr': gb_tpr,\n",
    "        'tnr': gb_tnr,\n",
    "        'fpr': gb_fpr,\n",
    "        'fnr': gb_fnr\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"  Gradient Boosting Accuracy: {gb_accuracy:.4f}, F1 Score: {gb_f1:.4f}, AUC: {gb_auc:.4f}\")\n",
    "\n",
    "    ### Support Vector Machine ###\n",
    "    # Instantiate SVM classifier with default hyperparameters\n",
    "    svm_model = SVC(kernel='rbf', probability=True, random_state=123)\n",
    "    \n",
    "    # Train SVM model\n",
    "    svm_model.fit(x_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    svm_pred = svm_model.predict(x_val_scaled)\n",
    "    svm_pred_prob = svm_model.predict_proba(x_val_scaled)[:, 1]\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    svm_accuracy = accuracy_score(y_val, svm_pred)\n",
    "    svm_f1 = f1_score(y_val, svm_pred)\n",
    "    svm_auc = roc_auc_score(y_val, svm_pred_prob)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, svm_pred).ravel()\n",
    "    svm_tpr = tp / (tp + fn)\n",
    "    svm_tnr = tn / (tn + fp)\n",
    "    svm_fpr = fp / (fp + tn)\n",
    "    svm_fnr = fn / (fn + tp)\n",
    "\n",
    "    # Record results\n",
    "    svm_metrics_list.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': svm_accuracy,\n",
    "        'f1_score': svm_f1,\n",
    "        'auc': svm_auc,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tpr': svm_tpr,\n",
    "        'tnr': svm_tnr,\n",
    "        'fpr': svm_fpr,\n",
    "        'fnr': svm_fnr\n",
    "    })\n",
    "    \n",
    "    print(f\"  SVM Accuracy: {svm_accuracy:.4f}, F1 Score: {svm_f1:.4f}, AUC: {svm_auc:.4f}\")\n",
    "    \n",
    "    fold += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "48298eef-4f2e-42c2-afa5-ce17a98ef2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loops, create DataFrames for the results\n",
    "nn_results = pd.DataFrame(nn_results_list)\n",
    "rf_results = pd.DataFrame(rf_metrics_list)\n",
    "svm_results = pd.DataFrame(svm_metrics_list)\n",
    "gb_results = pd.DataFrame(gb_metrics_list)\n",
    "\n",
    "# Aggregate neural network results over folds\n",
    "mean_nn_results = nn_results.groupby(['units1', 'units2', 'dropout_rate', 'batch_size', 'epochs', 'lr'], as_index=False).agg({\n",
    "    'val_loss': 'mean',\n",
    "    'val_accuracy': 'mean',\n",
    "    'f1_score': 'mean',\n",
    "    'auc': 'mean',\n",
    "    'tpr': 'mean',\n",
    "    'tnr': 'mean',\n",
    "    'fpr': 'mean',\n",
    "    'fnr': 'mean'\n",
    "}).sort_values(by='val_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "28f05cb9-9bcf-4649-bc0c-c2145e9a9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all the files\n",
    "nn_results.to_csv('nn_results.csv', index=False)  # Set index=False to avoid writing row numbers\n",
    "rf_results.to_csv('rf_results.csv', index=False)  # Set index=False to avoid writing row numbers\n",
    "svm_results.to_csv('svm_results.csv', index=False)  # Set index=False to avoid writing row numbers\n",
    "mean_nn_results.to_csv('mean_nn_results.csv', index=False)  # Set index=False to avoid writing row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e2a66617-8463-4e6d-a0b1-7b3d2b32def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_results.to_csv('mean_gb_results.csv', index=False)  # Set index=False to avoid writing row numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "45a6d867-86d0-4855-875c-cdf6edb66c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fold  accuracy  f1_score       auc  tp   tn  fp  fn       tpr       tnr  \\\n",
      "0     1  0.657895  0.599119  0.700650  68  107  28  63  0.519084  0.792593   \n",
      "1     2  0.691729  0.663934  0.752000  81  103  22  60  0.574468  0.824000   \n",
      "2     3  0.706767  0.669492  0.751077  79  109  31  47  0.626984  0.778571   \n",
      "3     4  0.706767  0.677686  0.762040  82  106  39  39  0.677686  0.731034   \n",
      "4     5  0.654135  0.646154  0.711016  84   90  30  62  0.575342  0.750000   \n",
      "\n",
      "        fpr       fnr  FPR_threshold  \n",
      "0  0.207407  0.480916       0.706707  \n",
      "1  0.176000  0.425532       0.683409  \n",
      "2  0.221429  0.373016       0.701180  \n",
      "3  0.268966  0.322314       0.702879  \n",
      "4  0.250000  0.424658       0.694702  \n"
     ]
    }
   ],
   "source": [
    "print(rf_results)\n",
    "# print(svm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0e8258c8-16ce-496a-88db-84e72f8abd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Mean Validation Metrics:\n",
      "   units1  units2  dropout_rate  batch_size  epochs     lr  val_loss  \\\n",
      "1      32      16           0.2          32      50  0.010  0.646355   \n",
      "5      64      16           0.2          32      50  0.010  0.647074   \n",
      "6      64      32           0.2          32      50  0.001  0.644838   \n",
      "7      64      32           0.2          32      50  0.010  0.645760   \n",
      "0      32      16           0.2          32      50  0.001  0.646922   \n",
      "2      32      32           0.2          32      50  0.001  0.649412   \n",
      "3      32      32           0.2          32      50  0.010  0.644242   \n",
      "4      64      16           0.2          32      50  0.001  0.640641   \n",
      "\n",
      "   val_accuracy  f1_score       auc       tpr       tnr       fpr       fnr  \n",
      "1      0.640602  0.625436  0.679094  0.605479  0.675889  0.324111  0.394521  \n",
      "5      0.636090  0.640879  0.682611  0.653927  0.621041  0.378959  0.346073  \n",
      "6      0.631579  0.626924  0.681231  0.622149  0.641780  0.358220  0.377851  \n",
      "7      0.631579  0.623046  0.680189  0.613234  0.648008  0.351992  0.386766  \n",
      "0      0.630827  0.611022  0.674021  0.587439  0.671603  0.328397  0.412561  \n",
      "2      0.629323  0.624187  0.677737  0.616917  0.643272  0.356728  0.383083  \n",
      "3      0.624812  0.621265  0.688389  0.617597  0.629272  0.370728  0.382403  \n",
      "4      0.621805  0.615150  0.681916  0.605194  0.638018  0.361982  0.394806  \n",
      "\n",
      "Random Forest Mean Validation Metrics:\n",
      "fold               3.000000\n",
      "accuracy           0.683459\n",
      "f1_score           0.651277\n",
      "auc                0.735357\n",
      "tp                78.800000\n",
      "tn               103.000000\n",
      "fp                30.000000\n",
      "fn                54.200000\n",
      "tpr                0.594713\n",
      "tnr                0.775240\n",
      "fpr                0.224760\n",
      "fnr                0.405287\n",
      "FPR_threshold      0.697775\n",
      "dtype: float64\n",
      "\n",
      "SVM Mean Validation Metrics:\n",
      "fold         3.000000\n",
      "accuracy     0.636090\n",
      "f1_score     0.638472\n",
      "auc          0.679154\n",
      "tp          85.600000\n",
      "tn          83.600000\n",
      "fp          49.400000\n",
      "fn          47.400000\n",
      "tpr          0.646163\n",
      "tnr          0.629300\n",
      "fpr          0.370700\n",
      "fnr          0.353837\n",
      "dtype: float64\n",
      "\n",
      "Gradient boosing Mean Validation Metrics:\n",
      "fold         3.000000\n",
      "accuracy     0.670677\n",
      "f1_score     0.651893\n",
      "auc          0.728806\n",
      "tp          82.200000\n",
      "tn          96.200000\n",
      "fp          36.800000\n",
      "fn          50.800000\n",
      "tpr          0.619390\n",
      "tnr          0.725432\n",
      "fpr          0.274568\n",
      "fnr          0.380610\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show the best hyperparameters\n",
    "print(\"\\nNeural Network Mean Validation Metrics:\")\n",
    "print(mean_nn_results)\n",
    "\n",
    "# Calculate mean metrics for Random Forest and SVM\n",
    "mean_rf_results = rf_results.mean()\n",
    "mean_svm_results = svm_results.mean()\n",
    "mean_gb_results = gb_results.mean()\n",
    "\n",
    "print(\"\\nRandom Forest Mean Validation Metrics:\")\n",
    "print(mean_rf_results)\n",
    "\n",
    "print(\"\\nSVM Mean Validation Metrics:\")\n",
    "print(mean_svm_results)\n",
    "\n",
    "print(\"\\nGradient boosing Mean Validation Metrics:\")\n",
    "print(mean_gb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d5b5a4a7-1e4c-40ab-8abb-94ff5de1fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forpred = pd.read_parquet('test_with_pairs_Clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3f34d6a9-8dba-470d-9df9-a09cb74054b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = forpred['pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6dc68b4b-5358-4065-aec3-1e148fa05cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = data.columns.drop(['Unnamed: 0', 'target', 'CADD_METSIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "30534e4f-bae1-48dc-a54a-1ac86fe58a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train on the full\n",
    "data = pd.read_csv('20240923_TrainingDataSet_Clean.csv')\n",
    "data['target'] = data['target'].replace(-1, 0)\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.dropna()\n",
    "x_all = data[feature_columns].values\n",
    "y_all = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0ac73916-a716-4924-b96c-8a38b705b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest model on all data\n",
    "final_rf_model = RandomForestClassifier(random_state=123, min_samples_split=5, max_depth=10, n_estimators=200)\n",
    "final_rf_model.fit(x_all, y_all)\n",
    "\n",
    "# Evaluate final Random Forest model\n",
    "rf_pred_all = final_rf_model.predict(x_all)\n",
    "rf_pred_prob_all = final_rf_model.predict_proba(x_all)[:, 1]\n",
    "\n",
    "rf_accuracy_all = accuracy_score(y_all, rf_pred_all)\n",
    "rf_f1_all = f1_score(y_all, rf_pred_all)\n",
    "rf_auc_all = roc_auc_score(y_all, rf_pred_prob_all)\n",
    "tn, fp, fn, tp = confusion_matrix(y_all, rf_pred_all).ravel()\n",
    "rf_tpr = tp / (tp + fn)\n",
    "rf_tnr = tn / (tn + fp)\n",
    "rf_fpr = fp / (fp + tn)\n",
    "rf_fnr = fn / (fn + tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9fb3c891-3006-437a-b3de-ddba840a6d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Importance\n",
      "11   Neighb_Rxn_MinP_METSIM    0.090316\n",
      "12       Neighb_TWAS_METSIM    0.089406\n",
      "5         Neighb_Rxn_N_CLSA    0.078090\n",
      "9           Min pval_METSIM    0.072111\n",
      "3      Neighb_Rxn_MinP_CLSA    0.069906\n",
      "4          Neighb_TWAS_CLSA    0.068230\n",
      "7     Neighb_Chem_TWAS_CLSA    0.061834\n",
      "2                 CADD_CLSA    0.060054\n",
      "13      Neighb_Rxn_N_METSIM    0.058214\n",
      "1                 TWAS_CLSA    0.057464\n",
      "0             Min pval_CLSA    0.050951\n",
      "6     Neighb_Chem_MinP_CLSA    0.050460\n",
      "10              TWAS_METSIM    0.048501\n",
      "15  Neighb_Chem_TWAS_METSIM    0.046055\n",
      "14  Neighb_Chem_MinP_METSIM    0.042857\n",
      "8        Neighb_Chem_N_CLSA    0.028620\n",
      "16     Neighb_Chem_N_METSIM    0.026929\n"
     ]
    }
   ],
   "source": [
    "# importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': final_rf_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance DataFrame\n",
    "print(importance_df)\n",
    "\n",
    "# Output as dataframe \n",
    "importance_df.to_csv('importance_results.csv', index=False)  # Set index=False to avoid writing row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "372c5292-8d04-4519-a8e5-a3c81f42462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forpred = forpred[feature_columns]\n",
    "rf_pred = final_rf_model.predict_proba(forpred.values)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c4cfcff8-4676-4046-ad0d-490c5561e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_custom_threshold = (rf_pred >= 0.697775).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b66d2fa6-3856-426a-9541-7ce56ce1b0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123930"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many positive associations there are\n",
    "sum(y_pred_custom_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4598d5b2-418f-4d16-8c88-138ddc74204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prediction = pd.DataFrame({\n",
    "    'Pairs': pairs,\n",
    "    'Prob': rf_pred,\n",
    "    'Class': y_pred_custom_threshold\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "436d4db7-02dc-4642-ac73-8db689bbcbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2426064"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d3f8ec72-9aa4-4c0b-868c-34052a23346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prediction.to_csv('20240924_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
